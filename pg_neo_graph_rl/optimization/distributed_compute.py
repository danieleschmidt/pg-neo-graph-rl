"""Distributed computing infrastructure for federated learning at scale."""
import time\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom typing import Dict, List, Any, Optional, Callable, Union, Tuple\nfrom dataclasses import dataclass\nimport threading\nimport multiprocessing as mp\nfrom queue import Queue\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import pmap, vmap\n\nfrom ..utils.logging import get_logger, get_performance_logger\nfrom ..core.types import GraphState\nfrom ..utils.exceptions import DistributedComputeError\nfrom ..monitoring.advanced_metrics import AdvancedMetricsCollector\n\nlogger = get_logger(__name__)\nperf_logger = get_performance_logger(__name__)\n\n@dataclass\nclass ComputeNode:\n    \"\"\"Represents a compute node in the distributed system.\"\"\"\n    node_id: str\n    cpu_count: int\n    memory_gb: float\n    gpu_count: int = 0\n    load_factor: float = 0.0\n    last_heartbeat: float = 0.0\n    is_active: bool = True\n    specialization: Optional[str] = None  # 'cpu', 'gpu', 'memory_intensive'\n\n@dataclass\nclass ComputeTask:\n    \"\"\"Represents a compute task to be executed.\"\"\"\n    task_id: str\n    task_type: str\n    data: Any\n    priority: int = 0\n    estimated_duration: float = 0.0\n    required_resources: Dict[str, Any] = None\n    callback: Optional[Callable] = None\n\nclass DistributedComputeManager:\n    \"\"\"Manages distributed computation across multiple nodes.\"\"\"\n    \n    def __init__(self, max_workers: int = None, enable_gpu: bool = True):\n        self.max_workers = max_workers or mp.cpu_count()\n        self.enable_gpu = enable_gpu and len(jax.devices('gpu')) > 0\n        \n        # Compute resources\n        self.thread_pool = ThreadPoolExecutor(max_workers=self.max_workers)\n        self.process_pool = ProcessPoolExecutor(max_workers=max(1, self.max_workers // 2))\n        \n        # Task management\n        self.task_queue = Queue()\n        self.active_tasks = {}\n        self.completed_tasks = {}\n        self.task_counter = 0\n        \n        # Node management\n        self.compute_nodes = {}\n        self.local_node = self._initialize_local_node()\n        \n        # Performance tracking\n        self.metrics_collector = AdvancedMetricsCollector()\n        \n        # Load balancing\n        self.load_balancer = LoadBalancer()\n        \n        # JAX setup for GPU acceleration\n        if self.enable_gpu:\n            self._setup_jax_distributed()\n            \n        logger.info(f\"Distributed compute manager initialized with {self.max_workers} workers\")\n        logger.info(f\"GPU acceleration: {'enabled' if self.enable_gpu else 'disabled'}\")\n    \n    def _initialize_local_node(self) -> ComputeNode:\n        \"\"\"Initialize local compute node information.\"\"\"\n        import psutil\n        \n        try:\n            memory_gb = psutil.virtual_memory().total / (1024**3)\n            cpu_count = mp.cpu_count()\n            gpu_count = len(jax.devices('gpu')) if self.enable_gpu else 0\n            \n            node = ComputeNode(\n                node_id=\"local\",\n                cpu_count=cpu_count,\n                memory_gb=memory_gb,\n                gpu_count=gpu_count,\n                last_heartbeat=time.time()\n            )\n            \n            self.compute_nodes[\"local\"] = node\n            return node\n            \n        except ImportError:\n            logger.warning(\"psutil not available, using default node config\")\n            return ComputeNode(\n                node_id=\"local\",\n                cpu_count=self.max_workers,\n                memory_gb=8.0,\n                gpu_count=1 if self.enable_gpu else 0,\n                last_heartbeat=time.time()\n            )\n    \n    def _setup_jax_distributed(self) -> None:\n        \"\"\"Setup JAX for distributed/multi-GPU computation.\"\"\"\n        try:\n            devices = jax.devices()\n            logger.info(f\"JAX devices available: {devices}\")\n            \n            # Setup for multi-device computation\n            self.device_count = len(devices)\n            if self.device_count > 1:\n                logger.info(f\"Multi-device computation enabled with {self.device_count} devices\")\n                \n        except Exception as e:\n            logger.warning(f\"JAX distributed setup error: {e}\")\n    \n    def submit_task(self, task_type: str, data: Any, priority: int = 0, \n                   node_preference: str = None) -> str:\n        \"\"\"Submit a compute task for execution.\"\"\"\n        task_id = f\"task_{self.task_counter}\"\n        self.task_counter += 1\n        \n        task = ComputeTask(\n            task_id=task_id,\n            task_type=task_type,\n            data=data,\n            priority=priority,\n            estimated_duration=self._estimate_task_duration(task_type, data)\n        )\n        \n        # Select optimal compute node\n        target_node = self._select_compute_node(task, node_preference)\n        \n        # Execute task\n        if task_type in ['graph_forward', 'gradient_computation', 'parameter_update']:\n            future = self._execute_ml_task(task, target_node)\n        elif task_type in ['data_processing', 'metric_aggregation']:\n            future = self._execute_data_task(task, target_node)\n        else:\n            future = self._execute_generic_task(task, target_node)\n        \n        self.active_tasks[task_id] = {\n            'task': task,\n            'future': future,\n            'node': target_node,\n            'start_time': time.time()\n        }\n        \n        logger.debug(f\"Submitted task {task_id} to node {target_node}\")\n        return task_id\n    \n    def _select_compute_node(self, task: ComputeTask, preference: str = None) -> str:\n        \"\"\"Select optimal compute node for task execution.\"\"\"\n        if preference and preference in self.compute_nodes:\n            if self.compute_nodes[preference].is_active:\n                return preference\n        \n        # Use load balancer to select node\n        return self.load_balancer.select_node(\n            task, \n            list(self.compute_nodes.values())\n        )\n    \n    def _estimate_task_duration(self, task_type: str, data: Any) -> float:\n        \"\"\"Estimate task execution duration.\"\"\"\n        base_times = {\n            'graph_forward': 0.01,\n            'gradient_computation': 0.05,\n            'parameter_update': 0.02,\n            'data_processing': 0.001,\n            'metric_aggregation': 0.005\n        }\n        \n        base_time = base_times.get(task_type, 0.01)\n        \n        # Adjust based on data size\n        if isinstance(data, (list, tuple)):\n            size_factor = len(data) / 1000\n        elif hasattr(data, 'shape'):\n            size_factor = np.prod(data.shape) / 10000\n        else:\n            size_factor = 1.0\n            \n        return base_time * (1 + size_factor)\n    \n    def _execute_ml_task(self, task: ComputeTask, node: str) -> Any:\n        \"\"\"Execute machine learning task with GPU acceleration if available.\"\"\"\n        if self.enable_gpu and task.task_type in ['graph_forward', 'gradient_computation']:\n            return self.thread_pool.submit(self._gpu_ml_task, task)\n        else:\n            return self.process_pool.submit(self._cpu_ml_task, task)\n    \n    def _execute_data_task(self, task: ComputeTask, node: str) -> Any:\n        \"\"\"Execute data processing task.\"\"\"\n        return self.thread_pool.submit(self._data_processing_task, task)\n    \n    def _execute_generic_task(self, task: ComputeTask, node: str) -> Any:\n        \"\"\"Execute generic compute task.\"\"\"\n        return self.thread_pool.submit(self._generic_task, task)\n    \n    def _gpu_ml_task(self, task: ComputeTask) -> Any:\n        \"\"\"Execute ML task on GPU.\"\"\"\n        start_time = time.time()\n        \n        try:\n            if task.task_type == 'graph_forward':\n                result = self._gpu_graph_forward(task.data)\n            elif task.task_type == 'gradient_computation':\n                result = self._gpu_gradient_computation(task.data)\n            else:\n                result = self._cpu_ml_task(task)  # Fallback\n                \n            duration = time.time() - start_time\n            self.metrics_collector.record('gpu_task_duration', duration)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"GPU task {task.task_id} failed: {e}\")\n            # Fallback to CPU\n            return self._cpu_ml_task(task)\n    \n    def _gpu_graph_forward(self, data: Dict[str, Any]) -> jnp.ndarray:\n        \"\"\"GPU-accelerated graph forward pass.\"\"\"\n        @jax.jit\n        def forward_pass(nodes, edges, adjacency):\n            # Simple graph convolution\n            degree = jnp.sum(adjacency, axis=1, keepdims=True)\n            degree = jnp.maximum(degree, 1.0)\n            \n            # Message passing\n            messages = jnp.dot(adjacency, nodes) / degree\n            \n            # Simple MLP layer\n            hidden = jax.nn.relu(messages @ jnp.ones((nodes.shape[1], 64)))\n            output = hidden @ jnp.ones((64, nodes.shape[1]))\n            \n            return output\n        \n        nodes = data['nodes']\n        adjacency = data['adjacency']\n        edges = data.get('edges', jnp.array([]))\n        \n        return forward_pass(nodes, edges, adjacency)\n    \n    def _gpu_gradient_computation(self, data: Dict[str, Any]) -> Dict[str, jnp.ndarray]:\n        \"\"\"GPU-accelerated gradient computation.\"\"\"\n        @jax.jit\n        def compute_gradients(params, inputs, targets):\n            def loss_fn(p):\n                # Simple loss computation\n                pred = inputs @ p\n                return jnp.mean((pred - targets) ** 2)\n            \n            return jax.grad(loss_fn)(params)\n        \n        params = data['parameters']\n        inputs = data['inputs']\n        targets = data['targets']\n        \n        gradients = compute_gradients(params, inputs, targets)\n        return {'gradients': gradients}\n    \n    def _cpu_ml_task(self, task: ComputeTask) -> Any:\n        \"\"\"Execute ML task on CPU.\"\"\"\n        start_time = time.time()\n        \n        try:\n            if task.task_type == 'graph_forward':\n                result = self._cpu_graph_forward(task.data)\n            elif task.task_type == 'gradient_computation':\n                result = self._cpu_gradient_computation(task.data)\n            elif task.task_type == 'parameter_update':\n                result = self._cpu_parameter_update(task.data)\n            else:\n                result = {'status': 'unknown_task_type'}\n            \n            duration = time.time() - start_time\n            self.metrics_collector.record('cpu_task_duration', duration)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"CPU ML task {task.task_id} failed: {e}\")\n            raise DistributedComputeError(f\"ML task failed: {e}\")\n    \n    def _cpu_graph_forward(self, data: Dict[str, Any]) -> np.ndarray:\n        \"\"\"CPU graph forward pass.\"\"\"\n        nodes = np.array(data['nodes'])\n        adjacency = np.array(data['adjacency'])\n        \n        # Simple graph convolution\n        degree = np.sum(adjacency, axis=1, keepdims=True)\n        degree = np.maximum(degree, 1.0)\n        \n        # Message passing\n        messages = np.dot(adjacency, nodes) / degree\n        \n        # Apply non-linearity\n        output = np.maximum(0, messages)  # ReLU activation\n        \n        return output\n    \n    def _cpu_gradient_computation(self, data: Dict[str, Any]) -> Dict[str, np.ndarray]:\n        \"\"\"CPU gradient computation.\"\"\"\n        # Simplified gradient computation\n        parameters = data['parameters']\n        inputs = data['inputs']\n        targets = data['targets']\n        \n        # Simple linear regression gradient\n        predictions = np.dot(inputs, parameters)\n        error = predictions - targets\n        gradients = np.dot(inputs.T, error) / len(targets)\n        \n        return {'gradients': gradients}\n    \n    def _cpu_parameter_update(self, data: Dict[str, Any]) -> Dict[str, np.ndarray]:\n        \"\"\"CPU parameter update.\"\"\"\n        parameters = data['parameters']\n        gradients = data['gradients']\n        learning_rate = data.get('learning_rate', 0.01)\n        \n        updated_params = parameters - learning_rate * gradients\n        \n        return {'parameters': updated_params}\n    \n    def _data_processing_task(self, task: ComputeTask) -> Any:\n        \"\"\"Execute data processing task.\"\"\"\n        start_time = time.time()\n        \n        try:\n            if task.task_type == 'metric_aggregation':\n                result = self._aggregate_metrics(task.data)\n            elif task.task_type == 'data_processing':\n                result = self._process_data(task.data)\n            else:\n                result = {'status': 'processed'}\n            \n            duration = time.time() - start_time\n            self.metrics_collector.record('data_task_duration', duration)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Data task {task.task_id} failed: {e}\")\n            raise DistributedComputeError(f\"Data task failed: {e}\")\n    \n    def _aggregate_metrics(self, data: List[Dict[str, float]]) -> Dict[str, float]:\n        \"\"\"Aggregate metrics from multiple sources.\"\"\"\n        if not data:\n            return {}\n        \n        # Collect all metric names\n        all_keys = set()\n        for item in data:\n            all_keys.update(item.keys())\n        \n        # Aggregate each metric\n        aggregated = {}\n        for key in all_keys:\n            values = [item[key] for item in data if key in item]\n            if values:\n                aggregated[key] = {\n                    'mean': np.mean(values),\n                    'std': np.std(values),\n                    'min': np.min(values),\n                    'max': np.max(values),\n                    'count': len(values)\n                }\n        \n        return aggregated\n    \n    def _process_data(self, data: Any) -> Any:\n        \"\"\"Generic data processing.\"\"\"\n        if isinstance(data, list):\n            # Process list items in parallel\n            return [self._process_single_item(item) for item in data]\n        else:\n            return self._process_single_item(data)\n    \n    def _process_single_item(self, item: Any) -> Any:\n        \"\"\"Process a single data item.\"\"\"\n        if isinstance(item, dict) and 'transform' in item:\n            # Apply transformation\n            transform = item['transform']\n            data = item['data']\n            \n            if transform == 'normalize':\n                return (data - np.mean(data)) / (np.std(data) + 1e-8)\n            elif transform == 'scale':\n                factor = item.get('factor', 1.0)\n                return data * factor\n            else:\n                return data\n        \n        return item\n    \n    def _generic_task(self, task: ComputeTask) -> Any:\n        \"\"\"Execute generic compute task.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Generic task execution\n            result = {'task_id': task.task_id, 'status': 'completed', 'data': task.data}\n            \n            duration = time.time() - start_time\n            self.metrics_collector.record('generic_task_duration', duration)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Generic task {task.task_id} failed: {e}\")\n            raise DistributedComputeError(f\"Generic task failed: {e}\")\n    \n    def get_task_result(self, task_id: str, timeout: float = None) -> Any:\n        \"\"\"Get result of a submitted task.\"\"\"\n        if task_id not in self.active_tasks:\n            if task_id in self.completed_tasks:\n                return self.completed_tasks[task_id]['result']\n            else:\n                raise ValueError(f\"Task {task_id} not found\")\n        \n        task_info = self.active_tasks[task_id]\n        future = task_info['future']\n        \n        try:\n            result = future.result(timeout=timeout)\n            \n            # Move to completed tasks\n            self.completed_tasks[task_id] = {\n                'task': task_info['task'],\n                'result': result,\n                'duration': time.time() - task_info['start_time'],\n                'node': task_info['node']\n            }\n            \n            del self.active_tasks[task_id]\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Task {task_id} execution failed: {e}\")\n            del self.active_tasks[task_id]\n            raise DistributedComputeError(f\"Task execution failed: {e}\")\n    \n    def get_system_status(self) -> Dict[str, Any]:\n        \"\"\"Get current system status.\"\"\"\n        return {\n            'active_tasks': len(self.active_tasks),\n            'completed_tasks': len(self.completed_tasks),\n            'compute_nodes': len(self.compute_nodes),\n            'thread_pool_active': self.thread_pool._threads,\n            'process_pool_active': len(self.process_pool._processes) if hasattr(self.process_pool, '_processes') else 0,\n            'gpu_enabled': self.enable_gpu,\n            'max_workers': self.max_workers\n        }\n    \n    def shutdown(self) -> None:\n        \"\"\"Shutdown distributed compute manager.\"\"\"\n        logger.info(\"Shutting down distributed compute manager...\")\n        \n        # Wait for active tasks to complete (with timeout)\n        for task_id in list(self.active_tasks.keys()):\n            try:\n                self.get_task_result(task_id, timeout=30.0)\n            except Exception as e:\n                logger.warning(f\"Task {task_id} did not complete cleanly: {e}\")\n        \n        # Shutdown thread and process pools\n        self.thread_pool.shutdown(wait=True)\n        self.process_pool.shutdown(wait=True)\n        \n        # Shutdown metrics collector\n        if hasattr(self.metrics_collector, 'shutdown'):\n            self.metrics_collector.shutdown()\n        \n        logger.info(\"Distributed compute manager shutdown complete\")\n\nclass LoadBalancer:\n    \"\"\"Load balancer for distributed compute nodes.\"\"\"\n    \n    def __init__(self, algorithm: str = 'least_loaded'):\n        self.algorithm = algorithm\n        self.node_loads = {}\n        self.last_selection = {}\n    \n    def select_node(self, task: ComputeTask, available_nodes: List[ComputeNode]) -> str:\n        \"\"\"Select optimal node for task execution.\"\"\"\n        if not available_nodes:\n            raise DistributedComputeError(\"No available compute nodes\")\n        \n        active_nodes = [node for node in available_nodes if node.is_active]\n        if not active_nodes:\n            raise DistributedComputeError(\"No active compute nodes\")\n        \n        if self.algorithm == 'least_loaded':\n            return self._select_least_loaded(active_nodes)\n        elif self.algorithm == 'round_robin':\n            return self._select_round_robin(active_nodes)\n        elif self.algorithm == 'resource_aware':\n            return self._select_resource_aware(task, active_nodes)\n        else:\n            # Default to first available node\n            return active_nodes[0].node_id\n    \n    def _select_least_loaded(self, nodes: List[ComputeNode]) -> str:\n        \"\"\"Select node with lowest load factor.\"\"\"\n        min_load = float('inf')\n        selected_node = nodes[0].node_id\n        \n        for node in nodes:\n            if node.load_factor < min_load:\n                min_load = node.load_factor\n                selected_node = node.node_id\n        \n        return selected_node\n    \n    def _select_round_robin(self, nodes: List[ComputeNode]) -> str:\n        \"\"\"Select node using round-robin algorithm.\"\"\"\n        node_ids = [node.node_id for node in nodes]\n        last_idx = self.last_selection.get('round_robin', -1)\n        next_idx = (last_idx + 1) % len(node_ids)\n        self.last_selection['round_robin'] = next_idx\n        return node_ids[next_idx]\n    \n    def _select_resource_aware(self, task: ComputeTask, nodes: List[ComputeNode]) -> str:\n        \"\"\"Select node based on resource requirements.\"\"\"\n        # Score nodes based on available resources\n        best_score = -1\n        selected_node = nodes[0].node_id\n        \n        for node in nodes:\n            score = self._calculate_node_score(task, node)\n            if score > best_score:\n                best_score = score\n                selected_node = node.node_id\n        \n        return selected_node\n    \n    def _calculate_node_score(self, task: ComputeTask, node: ComputeNode) -> float:\n        \"\"\"Calculate compatibility score between task and node.\"\"\"\n        score = 0.0\n        \n        # Prefer nodes with lower load\n        score += (1.0 - node.load_factor) * 0.4\n        \n        # Task-specific scoring\n        if task.task_type in ['graph_forward', 'gradient_computation'] and node.gpu_count > 0:\n            score += 0.3  # GPU tasks prefer GPU nodes\n        \n        if task.task_type == 'data_processing' and node.memory_gb > 16:\n            score += 0.2  # Memory-intensive tasks prefer high-memory nodes\n        \n        # CPU capacity\n        score += min(node.cpu_count / 8.0, 1.0) * 0.1\n        \n        return score\n    \n    def update_node_load(self, node_id: str, load_factor: float) -> None:\n        \"\"\"Update load factor for a node.\"\"\"\n        self.node_loads[node_id] = load_factor\n\n# Global distributed compute manager instance\n_global_compute_manager = None\n\ndef get_distributed_compute_manager(**kwargs) -> DistributedComputeManager:\n    \"\"\"Get or create global distributed compute manager.\"\"\"\n    global _global_compute_manager\n    if _global_compute_manager is None:\n        _global_compute_manager = DistributedComputeManager(**kwargs)\n    return _global_compute_manager